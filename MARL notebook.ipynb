{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b5ce7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18d95ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 4 # number of states\n",
    "n_agents = 3 # number of agents\n",
    "\n",
    "n_Us = [2] * n_agents # number of actions per agent (for now all agents have just two actions)\n",
    "Us = [range(n_Ui) for n_Ui in n_Us] # action spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eccb99a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_space = itertools.product(Ui, repeat=n_states)\n",
    "# policy[state] is a deterministic policy giving the action taken at state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20869806",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 # temp\n",
    "Qi_0 = np.zeros(shape=(n_states, n_Us[i]))\n",
    "# Qi_t[state, action] is the Q-factor for taking action at state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eacf7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_policy(action_space, n_states):\n",
    "    return tuple(np.random.choice(action_space, size=n_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09f65c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_state():\n",
    "    # initial state distribution\n",
    "    return np.random.choice(range(n_states)) # uniform over states for now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cc8f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(action_space, state):\n",
    "    '''uniformly choose from `action_space`'''\n",
    "    return np.random.choice(action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1faabd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_action(policy, state, action_space, rho):\n",
    "    '''follow random policy w/ prob `rho`; otherwise follow given policy'''\n",
    "    \n",
    "    if np.random.random() < rho:\n",
    "        return random_policy(action_space, state)\n",
    "    else:\n",
    "        return policy[state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a06e9d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cost_func(cost_matrix, i):\n",
    "    '''create a cost function : X, U -> cost from `cost_matrix`'''\n",
    "    \n",
    "    def cost_func(x, us):\n",
    "        u = us[i]\n",
    "        return cost_matrix[x,u]\n",
    "\n",
    "    return cost_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "006befe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize cost functions\n",
    "# note: here the cost functions don't depend on other agent's actions. \n",
    "C_matrices = [np.zeros((n_states, n_Us[0])), np.ones((n_states, n_Us[1])), np.random.random((n_states, n_Us[2]))]\n",
    "\n",
    "C_funcs = [create_cost_func(C_matrix, i) for i, C_matrix in enumerate(C_matrices)]\n",
    "\n",
    "# C_funcs[i](x, u) gives the cost function of agent i if action u is taken at state x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bae88def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp: parameters\n",
    "experimentation_prob = 0.1 # probability of experimentation at each action\n",
    "inertias = [0.05] * n_agents # inertias of each agent (\\lambda)\n",
    "betas = [0.9]*n_agents # discount factor\n",
    "n_exploration_phases = 100 # number of exploration phases\n",
    "T = 1_000 # length of exploration phase\n",
    "deltas = [0.05]*n_agents\n",
    "\n",
    "def transition_state(x, us):\n",
    "    '''returns next state given current state and action'''\n",
    "    \n",
    "    # for now, just cycle through states according to action of first agent\n",
    "    u = us[0]\n",
    "    return (x + u) % n_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a4a278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Qi(Qi, x_t, ui_t, cost_i, beta_i, alpha_i_n):\n",
    "    Qi_new = Qi\n",
    "    Qi_new[x_t, ui_t] = (1-alpha_i_n) * Qi[x_t, ui_t] + alpha_i_n * (cost_i + beta_i*np.min(Qi[x_t, :]))\n",
    "    \n",
    "    return Qi_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c1147c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_alpha(n):\n",
    "    return 1/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "25f7aa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_best_reply(Qi, policy, delta_i):\n",
    "    '''estimate whether a policy is approximately a best-response to the learned Q-factor'''\n",
    "    \n",
    "    policy_value = Qi[range(len(policy)), policy]\n",
    "    opt_value = np.min(Qi, axis = 1)\n",
    "    \n",
    "    return np.all(policy_value <= opt_value + delta_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cb6450b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "#def q_learning_alg1():\n",
    "\n",
    "# initialize Q_space? NOTE: did not account for compactness of space\n",
    "# initialize sequence {T_k}: length of exploration phase k\n",
    "# initialize exploration prob \\rho\n",
    "# initialize inertia \\lambda\n",
    "# initialize tolerance level for sub-optimality\n",
    "# initialize \\alpha^i_n sequence of step sizes\n",
    "\n",
    "# initialize policies for each agent (agent_policies[agent] gives agent's policy)\n",
    "agent_policies = [get_random_policy(Ui, n_states) for Ui in Us]\n",
    "\n",
    "# initialize Q-factors for each agent (Qs[agent] gives agent's Q-factor)\n",
    "Qs = [np.zeros(shape=(n_states, n_Ui)) for n_Ui in n_Us]\n",
    "\n",
    "x_0 = get_initial_state()\n",
    "x_t = x_0\n",
    "\n",
    "t = 0\n",
    "\n",
    "# iterate over exploration phases\n",
    "for k in range(n_exploration_phases):\n",
    "    \n",
    "    print(k)\n",
    "\n",
    "\n",
    "    # initialize n_ts number of visits to (x,u^i) in kth exploration phase up to t\n",
    "    # n_ts[i][x,u] gives the number of visits to (x, u^i) for agent i\n",
    "    n_ts = [np.zeros(shape=(n_states, n_Us[i])) for i in range(n_agents)]\n",
    "\n",
    "    # iterate over time in exploration phase k\n",
    "    for t in range(t, t + T):\n",
    "        #print(f'k={k}; t={t}')\n",
    "\n",
    "        # choose actions\n",
    "        actions_t = [randomize_action(agent_policies[i], x_t, Us[i], experimentation_prob) for i in range(n_agents)]\n",
    "        #print(f'actions: {actions_t}')\n",
    "\n",
    "        # receive costs\n",
    "        costs = [C_funcs[i](x_t, actions_t) for i in range(n_agents)]\n",
    "        #print(f'costs: {costs}')\n",
    "\n",
    "        # recive next state\n",
    "        next_state = transition_state(x_t, actions_t)\n",
    "        #print(f'next state: {next_state}')\n",
    "\n",
    "        # update n_ts number of visits to (x_t, u^i_t) in kth exploration phase up to t\n",
    "        for i in range(n_agents):\n",
    "            ui_t = actions_t[i]\n",
    "            n_ts[i][x_t, ui_t] += 1\n",
    "\n",
    "        # update Q-factors\n",
    "        for i in range(n_agents):\n",
    "\n",
    "            ui_t = actions_t[i]\n",
    "            cost_i = costs[i]\n",
    "            beta_i = betas[i]\n",
    "\n",
    "            alpha_i_n = calc_alpha(n_ts[i][x_t, ui_t])\n",
    "\n",
    "            Qs[i] = update_Qi(Qs[i], x_t, ui_t, cost_i, beta_i, alpha_i_n)\n",
    "\n",
    "\n",
    "\n",
    "        # update x_t\n",
    "        x_t = next_state\n",
    "\n",
    "     # calculate estimate of best reply policy space\n",
    "    for i in range(n_agents):\n",
    "        full_policy_space_i = itertools.product(Us[i], repeat=n_states)\n",
    "        br_policy_space_i = [policy for policy in full_policy_space_i if is_best_reply(Qs[i], policy, deltas[i])]\n",
    "\n",
    "        # if agent i's policy is not a best response replace it with a best response\n",
    "        if agent_policies[i] not in br_policy_space_i:\n",
    "            # with inertia, don't replace policy even if it's not a best response\n",
    "            if np.random.random() < 1 - inertias[i]:\n",
    "                agent_policies[i] = br_policy_space_i[np.random.choice(len(br_policy_space_i))]\n",
    "\n",
    "    t+=1 # increment to start on next t next exploration phase\n",
    "    #print()\n",
    "\n",
    "\n",
    "#print(Qs)\n",
    "# reset Q-factors to anything in Q_space. (perhapse project) [NOTE: i'm ignoring this for now]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "29f51fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]]),\n",
       " array([[10., 10.],\n",
       "        [10., 10.],\n",
       "        [10., 10.],\n",
       "        [10., 10.]]),\n",
       " array([[0.68231688, 0.89458779],\n",
       "        [6.46263277, 6.23610682],\n",
       "        [3.30249794, 2.85761639],\n",
       "        [7.20390153, 7.08968998]])]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "807412a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]]),\n",
       " array([[10., 10.],\n",
       "        [10., 10.],\n",
       "        [10., 10.],\n",
       "        [10., 10.]]),\n",
       " array([[0.68231688, 0.89458779],\n",
       "        [6.46263277, 6.23610682],\n",
       "        [3.30249794, 2.85761639],\n",
       "        [7.20390153, 7.08968998]])]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583901de",
   "metadata": {},
   "source": [
    "## Convergence to Equilibria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "912fefde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe6556",
   "metadata": {},
   "source": [
    "## Symmetric Prisoner's Dilemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cff266d",
   "metadata": {},
   "source": [
    "### Game Setup: A simple prisoner's dilemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "952649c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_state():\n",
    "    # initial state distribution\n",
    "    return 0 # (there are no states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf8a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_matrix = np.array([[0, -1], [-1, 1]])\n",
    "\n",
    "def reward(state, actions):\n",
    "    u0, u1 = actions\n",
    "    return reward_matrix[u0, u1]\n",
    "    \n",
    "\n",
    "reward_funcs = [reward, reward]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bb1b490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_state(x, us):\n",
    "    '''returns next state given current state and action'''\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3507a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0.751 # 0.51\n",
    "def calc_alpha(n):\n",
    "    return 1/(n**r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f57cf10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 1 # number of states\n",
    "n_agents = 2 # number of agents\n",
    "\n",
    "n_Us = [2] * n_agents # number of actions per agent (both agents have two actions)\n",
    "\n",
    "experimentation_probs = [0.1]*n_agents # probability of experimentation at each action (\\rho)\n",
    "inertias = [0.25, 0.75] # inertias of each agent (\\lambda)\n",
    "betas = [0.9]*n_agents # discount factor\n",
    "deltas = [0]*n_agents # (in paper it's 0) [this is critical; does T depend on this?]\n",
    "n_exploration_phases = 100 # number of exploration phases\n",
    "T = 1000 # length of exploration phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30afcead",
   "metadata": {},
   "source": [
    "### Run Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5a92811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_agent_learning import q_learning_alg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7229a6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▋                                                                                | 2/100 [00:00<00:12,  7.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# Qs, agent_policies, (policy_history, Qs_history, is_BR_history) = q_learning_alg1(n_Us, n_states, reward_funcs, betas,\n",
    "#                                                                     get_initial_state, transition_state, \n",
    "#                                                                     n_exploration_phases, T, experimentation_probs,\n",
    "#                                                                     calc_alpha, deltas, inertias)\n",
    "\n",
    "K, Qs, agent_policies, (policy_history, Qs_history, is_BR_history) = q_learning_alg1(n_Us, n_states, reward_funcs, betas,\n",
    "                                                                    get_initial_state, transition_state, \n",
    "                                                                    n_exploration_phases, T, experimentation_probs,\n",
    "                                                                    calc_alpha, deltas, inertias, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a08bb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.24883636, -1.19108541]]), array([[-0.34648114, -1.24413879]])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9587700",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 2\n",
    "agent_policy_space = list(itertools.product(range(n_actions), repeat=n_states))\n",
    "\n",
    "joint_policy_space = list(itertools.product(agent_policy_space, repeat=n_agents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52dd4a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end policies\n",
    "end_data = {p: [] for p in joint_policy_space}\n",
    "transition_data = {p: [] for p in joint_policy_space}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ac23225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [02:49<00:00,  5.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# disable inner tqdm\n",
    "from tqdm import tqdm\n",
    "from functools import partialmethod\n",
    "\n",
    "tqdm.__init__ = partialmethod(tqdm.__init__, disable=True)\n",
    "\n",
    "n_trials = 1000\n",
    "for _ in tqdm(range(n_trials), disable=False):\n",
    "    K, Qs, agent_policies, (policy_history, Qs_history, is_BR_history) = q_learning_alg1(n_Us, n_states, reward_funcs, betas,\n",
    "                                                                    get_initial_state, transition_state, \n",
    "                                                                    n_exploration_phases, T, experimentation_probs,\n",
    "                                                                    calc_alpha, deltas, inertias, early_stopping=True)\n",
    "\n",
    "    init_policy = tuple(policy_history[0])\n",
    "    end_data[init_policy].append(tuple(agent_policies))\n",
    "    \n",
    "    for p1, p2 in zip(policy_history[:-1], policy_history[1:]):\n",
    "        transition_data[tuple(p1)].append(tuple(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de126986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_policy =  ((0,), (0,))\n",
      "prob ending at ((0,), (0,)) = 1.0\n",
      "prob ending at ((0,), (1,)) = 0.0\n",
      "prob ending at ((1,), (0,)) = 0.0\n",
      "prob ending at ((1,), (1,)) = 0.0\n",
      "\n",
      "initial_policy =  ((0,), (1,))\n",
      "prob ending at ((0,), (0,)) = 0.26582278481012656\n",
      "prob ending at ((0,), (1,)) = 0.0\n",
      "prob ending at ((1,), (0,)) = 0.0\n",
      "prob ending at ((1,), (1,)) = 0.7341772151898734\n",
      "\n",
      "initial_policy =  ((1,), (0,))\n",
      "prob ending at ((0,), (0,)) = 0.7529880478087649\n",
      "prob ending at ((0,), (1,)) = 0.0\n",
      "prob ending at ((1,), (0,)) = 0.0\n",
      "prob ending at ((1,), (1,)) = 0.24701195219123506\n",
      "\n",
      "initial_policy =  ((1,), (1,))\n",
      "prob ending at ((0,), (0,)) = 0.0\n",
      "prob ending at ((0,), (1,)) = 0.0\n",
      "prob ending at ((1,), (0,)) = 0.0\n",
      "prob ending at ((1,), (1,)) = 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "markov_end_prob = {p1: {} for p1 in joint_policy_space}\n",
    "\n",
    "for p1 in joint_policy_space:\n",
    "    arr = np.array(end_data[p1])\n",
    "    print('initial_policy = ', p1)\n",
    "    for p2 in joint_policy_space:\n",
    "        P12 = np.average(np.all(arr==p2, axis=1))\n",
    "        markov_end_prob[p1][p2] = P12\n",
    "        print(f'prob ending at {p2} = {P12}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62ada193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_policy =  ((0,), (0,))\n",
      "prob transitioning to ((0,), (0,)) = 1\n",
      "prob transitioning to ((0,), (1,)) = 0\n",
      "prob transitioning to ((1,), (0,)) = 0\n",
      "prob transitioning to ((1,), (1,)) = 0\n",
      "\n",
      "initial_policy =  ((0,), (1,))\n",
      "prob transitioning to ((0,), (0,)) = 0.06896551724137931\n",
      "prob transitioning to ((0,), (1,)) = 0.17771883289124668\n",
      "prob transitioning to ((1,), (0,)) = 0.19363395225464192\n",
      "prob transitioning to ((1,), (1,)) = 0.5596816976127321\n",
      "\n",
      "initial_policy =  ((1,), (0,))\n",
      "prob transitioning to ((0,), (0,)) = 0.5794871794871795\n",
      "prob transitioning to ((0,), (1,)) = 0.18717948717948718\n",
      "prob transitioning to ((1,), (0,)) = 0.16923076923076924\n",
      "prob transitioning to ((1,), (1,)) = 0.0641025641025641\n",
      "\n",
      "initial_policy =  ((1,), (1,))\n",
      "prob transitioning to ((0,), (0,)) = 0\n",
      "prob transitioning to ((0,), (1,)) = 0\n",
      "prob transitioning to ((1,), (0,)) = 0\n",
      "prob transitioning to ((1,), (1,)) = 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "markov_transition_prob = {p1: {} for p1 in joint_policy_space}\n",
    "\n",
    "for p1 in joint_policy_space:\n",
    "    arr = np.array(transition_data[p1])\n",
    "    print('initial_policy = ', p1)\n",
    "    if len(arr) > 0:\n",
    "        for p2 in joint_policy_space:\n",
    "            P12 = np.average(np.all(arr==p2, axis=1))\n",
    "            markov_transition_prob[p1][p2] = P12\n",
    "            print(f'prob transitioning to {p2} = {P12}')\n",
    "    else:\n",
    "        for p2 in joint_policy_space:\n",
    "            P12 = 1 if p1==p2 else 0\n",
    "            markov_transition_prob[p1][p2] = P12\n",
    "            print(f'prob transitioning to {p2} = {P12}')\n",
    "        \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef782c9",
   "metadata": {},
   "source": [
    "starting at an equilibrium => remain at that equilibrium \\\n",
    "starting at non-equilibrium => end up at either equilibrium w/ equal probability \\\n",
    "(as expected / BR graph is symmetric w.r.t. team optimal equilibrium and nash equilibrium)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "43b4a944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 0.  , 0.  , 0.  ],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.  , 0.  , 0.  , 1.  ]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# theoretical transition matrix from BR graph (inertia=0.5)\n",
    "markov_matrix = np.array([[1,0,0,0], [0.25]*4, [0.25]*4, [0,0,0,1]])\n",
    "markov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c43d85fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.06896552, 0.17771883, 0.19363395, 0.5596817 ],\n",
       "       [0.57948718, 0.18717949, 0.16923077, 0.06410256],\n",
       "       [0.        , 0.        , 0.        , 1.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# empirical transition matrix\n",
    "np.array([[markov_transition_prob[p1][p2] for p2 in joint_policy_space] for p1 in joint_policy_space])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f5fbe8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [5.00000000e-01, 3.94430453e-31, 3.94430453e-31, 5.00000000e-01],\n",
       "       [5.00000000e-01, 3.94430453e-31, 3.94430453e-31, 5.00000000e-01],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# theoretical end probabilities\n",
    "np.linalg.matrix_power(markov_matrix, 100) # ~ lim T^N (N-> oo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4d9d260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.26582278, 0.        , 0.        , 0.73417722],\n",
       "       [0.75298805, 0.        , 0.        , 0.24701195],\n",
       "       [0.        , 0.        , 0.        , 1.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[markov_end_prob[p1][p2] for p2 in joint_policy_space] for p1 in joint_policy_space])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c66c5e6",
   "metadata": {},
   "source": [
    "## Problem 2: Asymetric Prisoner's Dilemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe67356",
   "metadata": {},
   "source": [
    "### Game setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "7f46a2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just redefine reward funcs\n",
    "\n",
    "reward_matrix = np.array([[0, 0], [-1, 1]])\n",
    "\n",
    "def reward(state, actions):\n",
    "    u0, u1 = actions\n",
    "    return reward_matrix[u0, u1]\n",
    "    \n",
    "\n",
    "reward_funcs = [reward, reward]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "b22ba773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also need delta to be strictly positive, otherwise will transition even if at equilibrium\n",
    "deltas = [0.05]*n_agents # (in paper it's 0) [this is critical; does T depend on this?]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f8ed9e",
   "metadata": {},
   "source": [
    "### Run Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f29e2b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 2\n",
    "agent_policy_space = list(itertools.product(range(n_actions), repeat=n_states))\n",
    "\n",
    "joint_policy_space = list(itertools.product(agent_policy_space, repeat=n_agents))\n",
    "\n",
    "# end policies\n",
    "end_data = {p: [] for p in joint_policy_space}\n",
    "transition_data = {p: [] for p in joint_policy_space}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "14f85a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1000/1000 [03:41<00:00,  4.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# disable inner tqdm\n",
    "from tqdm import tqdm\n",
    "from functools import partialmethod\n",
    "\n",
    "tqdm.__init__ = partialmethod(tqdm.__init__, disable=True)\n",
    "\n",
    "n_trials = 1000\n",
    "for _ in tqdm(range(n_trials), disable=False):\n",
    "    K, Qs, agent_policies, (policy_history, Qs_history, is_BR_history) = q_learning_alg1(n_Us, n_states, reward_funcs, betas,\n",
    "                                                                    get_initial_state, transition_state, \n",
    "                                                                    n_exploration_phases, T, experimentation_probs,\n",
    "                                                                    calc_alpha, deltas, inertias, early_stopping=True)\n",
    "\n",
    "    init_policy = tuple(policy_history[0])\n",
    "    end_data[init_policy].append(tuple(agent_policies))\n",
    "    \n",
    "    for p1, p2 in zip(policy_history[:-1], policy_history[1:]):\n",
    "        transition_data[tuple(p1)].append(tuple(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "71a96225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_policy =  ((0,), (0,))\n",
      "prob ending at ((0,), (0,)) = 0.18875502008032127\n",
      "prob ending at ((0,), (1,)) = 0.0\n",
      "prob ending at ((1,), (0,)) = 0.0\n",
      "prob ending at ((1,), (1,)) = 0.8112449799196787\n",
      "\n",
      "initial_policy =  ((0,), (1,))\n",
      "prob ending at ((0,), (0,)) = 0.0\n",
      "prob ending at ((0,), (1,)) = 0.0\n",
      "prob ending at ((1,), (0,)) = 0.0\n",
      "prob ending at ((1,), (1,)) = 1.0\n",
      "\n",
      "initial_policy =  ((1,), (0,))\n",
      "prob ending at ((0,), (0,)) = 0.045454545454545456\n",
      "prob ending at ((0,), (1,)) = 0.0\n",
      "prob ending at ((1,), (0,)) = 0.0\n",
      "prob ending at ((1,), (1,)) = 0.9545454545454546\n",
      "\n",
      "initial_policy =  ((1,), (1,))\n",
      "prob ending at ((0,), (0,)) = 0.0\n",
      "prob ending at ((0,), (1,)) = 0.0\n",
      "prob ending at ((1,), (0,)) = 0.0\n",
      "prob ending at ((1,), (1,)) = 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate frequency/probability of ending at each policy given a starting policy\n",
    "\n",
    "markov_end_prob = {p1: {} for p1 in joint_policy_space}\n",
    "\n",
    "for p1 in joint_policy_space:\n",
    "    arr = np.array(end_data[p1])\n",
    "    print('initial_policy = ', p1)\n",
    "    for p2 in joint_policy_space:\n",
    "        P12 = np.average(np.all(arr==p2, axis=1))\n",
    "        markov_end_prob[p1][p2] = P12\n",
    "        print(f'prob ending at {p2} = {P12}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "5b21dbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_policy =  ((0,), (0,))\n",
      "prob transitioning to ((0,), (0,)) = 0.4847328244274809\n",
      "prob transitioning to ((0,), (1,)) = 0.5152671755725191\n",
      "prob transitioning to ((1,), (0,)) = 0.0\n",
      "prob transitioning to ((1,), (1,)) = 0.0\n",
      "\n",
      "initial_policy =  ((0,), (1,))\n",
      "prob transitioning to ((0,), (0,)) = 0.0\n",
      "prob transitioning to ((0,), (1,)) = 0.46562786434463793\n",
      "prob transitioning to ((1,), (0,)) = 0.0\n",
      "prob transitioning to ((1,), (1,)) = 0.534372135655362\n",
      "\n",
      "initial_policy =  ((1,), (0,))\n",
      "prob transitioning to ((0,), (0,)) = 0.215633423180593\n",
      "prob transitioning to ((0,), (1,)) = 0.23450134770889489\n",
      "prob transitioning to ((1,), (0,)) = 0.2884097035040431\n",
      "prob transitioning to ((1,), (1,)) = 0.261455525606469\n",
      "\n",
      "initial_policy =  ((1,), (1,))\n",
      "prob transitioning to ((0,), (0,)) = 0\n",
      "prob transitioning to ((0,), (1,)) = 0\n",
      "prob transitioning to ((1,), (0,)) = 0\n",
      "prob transitioning to ((1,), (1,)) = 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate frequency/probability of transitioning to each policy given a starting policy\n",
    "\n",
    "\n",
    "markov_transition_prob = {p1: {} for p1 in joint_policy_space}\n",
    "\n",
    "for p1 in joint_policy_space:\n",
    "    arr = np.array(transition_data[p1])\n",
    "    print('initial_policy = ', p1)\n",
    "    if len(arr) > 0:\n",
    "        for p2 in joint_policy_space:\n",
    "            P12 = np.average(np.all(arr==p2, axis=1))\n",
    "            markov_transition_prob[p1][p2] = P12\n",
    "            print(f'prob transitioning to {p2} = {P12}')\n",
    "    else:\n",
    "        for p2 in joint_policy_space:\n",
    "            P12 = 1 if p1==p2 else 0\n",
    "            markov_transition_prob[p1][p2] = P12\n",
    "            print(f'prob transitioning to {p2} = {P12}')\n",
    "        \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0b91ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.5 , 0.  , 0.5 ],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.  , 0.  , 0.  , 1.  ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# theoretical transition matrix from BR graph (inertia=0.5)\n",
    "markov_matrix = np.array([[1,0,0,0], [0, 0.5, 0, 0.5], [0.25]*4, [0,0,0,1]])\n",
    "markov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ed639d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48473282, 0.51526718, 0.        , 0.        ],\n",
       "       [0.        , 0.46562786, 0.        , 0.53437214],\n",
       "       [0.21563342, 0.23450135, 0.2884097 , 0.26145553],\n",
       "       [0.        , 0.        , 0.        , 1.        ]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# empirical transition matrix\n",
    "np.array([[markov_transition_prob[p1][p2] for p2 in joint_policy_space] for p1 in joint_policy_space])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "ea66cea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 7.88860905e-31, 0.00000000e+00, 1.00000000e+00],\n",
       "       [3.33333333e-01, 7.88860905e-31, 6.22301528e-61, 6.66666667e-01],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# theoretical end probabilities\n",
    "np.linalg.matrix_power(markov_matrix, 100) # ~ lim T^N (N-> oo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "d6cbb06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18875502, 0.        , 0.        , 0.81124498],\n",
       "       [0.        , 0.        , 0.        , 1.        ],\n",
       "       [0.04545455, 0.        , 0.        , 0.95454545],\n",
       "       [0.        , 0.        , 0.        , 1.        ]])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[markov_end_prob[p1][p2] for p2 in joint_policy_space] for p1 in joint_policy_space])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a300b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dd86d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1.0 & 0.0 & 0.0 & 0.0\\\\0.0 & 0.5 & 0.0 & 0.5\\\\0.25 & 0.25 & 0.25 & 0.25\\\\0.0 & 0.0 & 0.0 & 1.0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[ 1.0,  0.0,  0.0,  0.0],\n",
       "[ 0.0,  0.5,  0.0,  0.5],\n",
       "[0.25, 0.25, 0.25, 0.25],\n",
       "[ 0.0,  0.0,  0.0,  1.0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markov_matrix_sym = sympy.Matrix(markov_matrix)\n",
    "markov_matrix_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e08c6310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1.0 & 0 & 0 & 0\\\\0 & 7.8886 \\cdot 10^{-31} & 0 & 1.0\\\\0.33333 & 7.8886 \\cdot 10^{-31} & 6.223 \\cdot 10^{-61} & 0.66667\\\\0 & 0 & 0 & 1.0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[    1.0,          0,         0,       0],\n",
       "[      0, 7.8886e-31,         0,     1.0],\n",
       "[0.33333, 7.8886e-31, 6.223e-61, 0.66667],\n",
       "[      0,          0,         0,     1.0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(markov_matrix_sym**100).evalf(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f62a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(markov_matrix_sym**100).evalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61d72691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}p_{0} & p_{1} & p_{2} & p_{3}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([[p0, p1, p2, p3]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invar_meas = sympy.Matrix(sympy.symbols('p0:4')).T\n",
    "invar_meas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f902d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{p1: 0.0, p2: 0.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sympy.solve(invar_meas@markov_matrix_sym - invar_meas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42b9a3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0.25 p_{2} & - 0.5 p_{1} + 0.25 p_{2} & - 0.75 p_{2} & 0.5 p_{1} + 0.25 p_{2}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([[0.25*p2, -0.5*p1 + 0.25*p2, -0.75*p2, 0.5*p1 + 0.25*p2]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invar_meas@markov_matrix_sym - invar_meas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b072fe22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0,), (0,)), ((0,), (1,)), ((1,), (0,)), ((1,), (1,))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_policy_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58ffac73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,), (1,)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_policy_space"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Quantized Decentralized Multi-Agent Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from quantization import DiscreteSpace, ContinuousInterval, UniformQuantizer, NullQuantizer, QuantizedPolicy, TransitionKernel\n",
    "from cts_multi_agent_learning import quantized_q_learning_alg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trivial Single-Agent 'Finite' State and Action\n",
    "A simple MDP with a single state and two actions, one good one bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = DiscreteSpace([0]) # single state\n",
    "action_space = DiscreteSpace([0, 1]) # two actions\n",
    "\n",
    "state_quantizer = NullQuantizer(state_space)\n",
    "action_quantizer = NullQuantizer(action_space)\n",
    "\n",
    "q_policy = QuantizedPolicy(state_quantizer, action_quantizer, index_policy='random_init', exploration_prob=0.5)\n",
    "quantized_agent_policies = [q_policy]\n",
    "\n",
    "def transition_func(x, us):\n",
    "    return 0\n",
    "\n",
    "transition_kernel = TransitionKernel(state_space, [action_space], transition_func)\n",
    "\n",
    "def reward_func(x, us):\n",
    "    u = us[0]\n",
    "    if u==0:\n",
    "        return 0\n",
    "    elif u==1:\n",
    "        return 1\n",
    "    else:\n",
    "        raise ValueError('received action outside expected range')\n",
    "\n",
    "reward_funcs = [reward_func]\n",
    "\n",
    "betas = [0.5]\n",
    "\n",
    "T = int(1e5)\n",
    "\n",
    "alpha_func = lambda n: 1/(n + 1)\n",
    "\n",
    "n_exploration_phases = 1\n",
    "\n",
    "deltas = [1e-4]\n",
    "\n",
    "inertias = [0.5]\n",
    "\n",
    "get_initial_state = lambda: 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48cf11cbc0684d3a934689e4a24b3668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Qs, quantized_agent_policies, history = quantized_q_learning_alg(quantized_agent_policies, transition_kernel, get_initial_state,\n",
    "                reward_funcs, betas, T, alpha_func, n_exploration_phases, deltas, inertias, early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.99177106, 1.99175108]])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_agent_policies[0].get_policy_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slightly Less Trivial Single-Agent 'Finite' State and Action Spaces\n",
    "\n",
    "An MDP with a 'good' state and a 'bad' state, and two actions each of which causes a deterministic transition to one of the states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = DiscreteSpace(['good', 'bad']) # two state\n",
    "action_space = DiscreteSpace(['a', 'b']) # two actions\n",
    "\n",
    "state_quantizer = NullQuantizer(state_space)\n",
    "action_quantizer = NullQuantizer(action_space)\n",
    "\n",
    "q_policy = QuantizedPolicy(state_quantizer, action_quantizer, index_policy='random_init', exploration_prob=0.5)\n",
    "quantized_agent_policies = [q_policy]\n",
    "\n",
    "def transition_func(x, us):\n",
    "    u = us[0]\n",
    "    if u == 'a':\n",
    "        return 'good'\n",
    "    elif u=='b':\n",
    "        return 'bad'\n",
    "    else:\n",
    "        raise ValueError('received unexpected action')\n",
    "\n",
    "transition_kernel = TransitionKernel(state_space, [action_space], transition_func)\n",
    "\n",
    "def reward_func(x, us):\n",
    "    if x=='good':\n",
    "        return 1\n",
    "    elif x=='bad':\n",
    "        return 0\n",
    "    else:\n",
    "        raise ValueError('received state/action outside expected range')\n",
    "\n",
    "reward_funcs = [reward_func]\n",
    "\n",
    "betas = [0.25]\n",
    "\n",
    "T = int(1e5)\n",
    "\n",
    "alpha_func = lambda n: 1/(n + 1)\n",
    "\n",
    "n_exploration_phases = 1\n",
    "\n",
    "deltas = [1e-4]\n",
    "\n",
    "inertias = [0.5]\n",
    "\n",
    "get_initial_state = lambda: 'good'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f65075fbb4c4eafa06756d9ca2ba30f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Qs, quantized_agent_policies, history = quantized_q_learning_alg(quantized_agent_policies, transition_kernel, get_initial_state,\n",
    "                reward_funcs, betas, T, alpha_func, n_exploration_phases, deltas, inertias, early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.33296191, 1.08295976],\n",
       "        [0.33297202, 0.08295468]])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'good': 'a', 'bad': 'a'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_agent_policies[0].get_policy_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-agent Team (coordinate actions), 'Finite' State and action space\n",
    "\n",
    "A 2-agent generalization of the above where transition to the good state occurs of the agents coordinate (play the same action) and transition to the bad state occurs otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = DiscreteSpace(['good', 'bad']) # two state\n",
    "action_space1 = DiscreteSpace(['a', 'b']) # two actions\n",
    "action_space2 = DiscreteSpace(['a', 'b']) # two actions\n",
    "\n",
    "state_quantizer1 = NullQuantizer(state_space)\n",
    "state_quantizer2 = NullQuantizer(state_space)\n",
    "\n",
    "action_quantizer1 = NullQuantizer(action_space1)\n",
    "action_quantizer2 = NullQuantizer(action_space2)\n",
    "\n",
    "q_policy1 = QuantizedPolicy(state_quantizer1, action_quantizer1, index_policy='random_init', exploration_prob=0.25)\n",
    "q_policy2 = QuantizedPolicy(state_quantizer2, action_quantizer2, index_policy='random_init', exploration_prob=0.25)\n",
    "\n",
    "quantized_agent_policies = [q_policy1, q_policy2]\n",
    "\n",
    "def transition_func(x, us):\n",
    "    u1, u2 = us\n",
    "    \n",
    "    if u1 == u2:\n",
    "        return 'good'\n",
    "    elif u1 != u2:\n",
    "        return 'bad'\n",
    "    else:\n",
    "        raise ValueError('received unexpected action')\n",
    "\n",
    "transition_kernel = TransitionKernel(state_space, [action_space1, action_space2], transition_func)\n",
    "\n",
    "def reward_func(x, us):\n",
    "    if x=='good':\n",
    "        return 1\n",
    "    elif x=='bad':\n",
    "        return 0\n",
    "    else:\n",
    "        raise ValueError('received state/action outside expected range')\n",
    "\n",
    "reward_funcs = [reward_func, reward_func]\n",
    "\n",
    "betas = [0.25]*2\n",
    "\n",
    "T = int(5e4)\n",
    "\n",
    "alpha_func = lambda n: 1/(n + 1)\n",
    "\n",
    "n_exploration_phases = 1\n",
    "\n",
    "deltas = [1e-4]*2\n",
    "\n",
    "inertias = [0.5]*2\n",
    "\n",
    "get_initial_state = lambda: 'good'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33c05c25c214b9d9fa21e15e1f35c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Qs, quantized_agent_policies, history = quantized_q_learning_alg(quantized_agent_policies, transition_kernel, get_initial_state,\n",
    "                reward_funcs, betas, T, alpha_func, n_exploration_phases, deltas, inertias, early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.0513329 , 1.31955168],\n",
       "        [0.16692366, 0.18329821]]),\n",
       " array([[1.28893766, 1.10327584],\n",
       "        [0.10187907, 0.29034376]])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'good': 'a', 'bad': 'b'}, {'good': 'b', 'bad': 'b'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_agent_policies[0].get_policy_map(), quantized_agent_policies[1].get_policy_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous State-Space Single-Agent\n",
    "A continuous space MDP over $\\mathbb{X} = [0,1]$ and $\\mathbb{U} = \\{-1, 1\\}$ where actions either transition the state forward or backward. A larger state value corresponds to higher reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = ContinuousInterval(0, 1) # single state\n",
    "action_space = DiscreteSpace([-1, 1]) # two actions\n",
    "\n",
    "state_quantizer = UniformQuantizer(state_space, n_bins=10)\n",
    "action_quantizer = NullQuantizer(action_space)\n",
    "\n",
    "q_policy = QuantizedPolicy(state_quantizer, action_quantizer, index_policy='random_init', exploration_prob=0.5)\n",
    "quantized_agent_policies = [q_policy]\n",
    "\n",
    "def transition_func(x, us):\n",
    "    u = us[0]\n",
    "    x_next = np.clip(x + 0.1*u, 0, 1)\n",
    "    return x_next\n",
    "\n",
    "transition_kernel = TransitionKernel(state_space, [action_space], transition_func)\n",
    "\n",
    "def reward_func(x, us):\n",
    "    return x # reward high state values\n",
    "\n",
    "reward_funcs = [reward_func]\n",
    "\n",
    "betas = [0.25]\n",
    "\n",
    "T = int(1e5)\n",
    "\n",
    "alpha_func = lambda n: 1/(n + 1)\n",
    "\n",
    "n_exploration_phases = 1\n",
    "\n",
    "deltas = [1e-4]\n",
    "\n",
    "inertias = [0.5]\n",
    "\n",
    "get_initial_state = lambda: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee091821372643e1878ab4306511fe34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Qs, quantized_agent_policies, history = quantized_q_learning_alg(quantized_agent_policies, transition_kernel, get_initial_state,\n",
    "                reward_funcs, betas, T, alpha_func, n_exploration_phases, deltas, inertias, early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.0108065 , 0.04384476],\n",
       "        [0.11078582, 0.17665118],\n",
       "        [0.24383492, 0.30907713],\n",
       "        [0.37664552, 0.4412434 ],\n",
       "        [0.53842975, 0.57598689],\n",
       "        [0.76172575, 0.81857168],\n",
       "        [0.9027839 , 0.97159355],\n",
       "        [1.04097226, 1.09899312],\n",
       "        [1.17097353, 1.21966586],\n",
       "        [1.29793467, 1.31495625]])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 1,\n",
       " 0.1111111111111111: 1,\n",
       " 0.2222222222222222: -1,\n",
       " 0.3333333333333333: -1,\n",
       " 0.4444444444444444: -1,\n",
       " 0.5555555555555556: 1,\n",
       " 0.6666666666666666: 1,\n",
       " 0.7777777777777777: -1,\n",
       " 0.8888888888888888: -1,\n",
       " 1.0: -1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_agent_policies[0].get_policy_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous State-Space Single-Agent\n",
    "\n",
    "Similar to the above except the cost measures the distance from $x = 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = ContinuousInterval(0, 1) # single state\n",
    "action_space = DiscreteSpace([-1, 1]) # two actions\n",
    "\n",
    "state_quantizer = UniformQuantizer(state_space, n_bins=10)\n",
    "action_quantizer = NullQuantizer(action_space)\n",
    "\n",
    "q_policy = QuantizedPolicy(state_quantizer, action_quantizer, index_policy='random_init', exploration_prob=0.5)\n",
    "quantized_agent_policies = [q_policy]\n",
    "\n",
    "def transition_func(x, us):\n",
    "    u = us[0]\n",
    "    x_next = np.clip(x + 0.1*u, 0, 1)\n",
    "    return x_next\n",
    "\n",
    "transition_kernel = TransitionKernel(state_space, [action_space], transition_func)\n",
    "\n",
    "def reward_func(x, us):\n",
    "    return - np.abs(x - 0.5) # reward state values near 0.5\n",
    "\n",
    "reward_funcs = [reward_func]\n",
    "\n",
    "betas = [0.25]\n",
    "\n",
    "T = int(1e5)\n",
    "\n",
    "alpha_func = lambda n: 1/(n + 1)\n",
    "\n",
    "n_exploration_phases = 1\n",
    "\n",
    "deltas = [1e-4]\n",
    "\n",
    "inertias = [0.5]\n",
    "\n",
    "get_initial_state = lambda: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1232bd04343e4a418f0c97a9acfebd7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Qs, quantized_agent_policies, history = quantized_q_learning_alg(quantized_agent_policies, transition_kernel, get_initial_state,\n",
    "                reward_funcs, betas, T, alpha_func, n_exploration_phases, deltas, inertias, early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.64939519, -0.61987103],\n",
       "        [-0.55111295, -0.48794686],\n",
       "        [-0.42086477, -0.35599115],\n",
       "        [-0.28781956, -0.22744549],\n",
       "        [-0.13746208, -0.11201001],\n",
       "        [-0.10267158, -0.12619157],\n",
       "        [-0.22518406, -0.28814459],\n",
       "        [-0.35583935, -0.42146178],\n",
       "        [-0.48838033, -0.55489541],\n",
       "        [-0.62154145, -0.654924  ]])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 1,\n",
       " 0.1111111111111111: 1,\n",
       " 0.2222222222222222: 1,\n",
       " 0.3333333333333333: 1,\n",
       " 0.4444444444444444: 1,\n",
       " 0.5555555555555556: -1,\n",
       " 0.6666666666666666: -1,\n",
       " 0.7777777777777777: -1,\n",
       " 0.8888888888888888: -1,\n",
       " 1.0: -1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_agent_policies[0].get_policy_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous State-Space Two-Agent\n",
    "\n",
    "A 2-agent Generalization of the above where coordination causes moving forward in the state space and a lack of coordination causes moving backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = ContinuousInterval(0, 1) # single state\n",
    "action_space1 = DiscreteSpace([-1, 1]) # two actions\n",
    "action_space2 = DiscreteSpace([-1, 1]) # two actions\n",
    "\n",
    "state_quantizer1 = UniformQuantizer(state_space, n_bins=10)\n",
    "state_quantizer2 = UniformQuantizer(state_space, n_bins=10)\n",
    "\n",
    "action_quantizer1 = NullQuantizer(action_space1)\n",
    "action_quantizer2 = NullQuantizer(action_space2)\n",
    "\n",
    "q_policy1 = QuantizedPolicy(state_quantizer1, action_quantizer1, index_policy='random_init', exploration_prob=0.25)\n",
    "q_policy2 = QuantizedPolicy(state_quantizer2, action_quantizer2, index_policy='random_init', exploration_prob=0.25)\n",
    "\n",
    "quantized_agent_policies = [q_policy1, q_policy2]\n",
    "\n",
    "def transition_func(x, us):\n",
    "    u1, u2 = us\n",
    "    d = 1 if u1==u2 else -1 # go forward if coordinating, backwards otherwise\n",
    "    x_next = np.clip(x + 0.1*d, 0, 1)\n",
    "    return x_next\n",
    "\n",
    "transition_kernel = TransitionKernel(state_space, [action_space1, action_space2], transition_func)\n",
    "\n",
    "def reward_func(x, us):\n",
    "    return - np.abs(x - 0.5) # reward state values near 0.5\n",
    "\n",
    "reward_funcs = [reward_func, reward_func]\n",
    "\n",
    "betas = [0.25]*2\n",
    "\n",
    "T = int(1e5)\n",
    "\n",
    "alpha_func = lambda n: 1/(n + 1)\n",
    "\n",
    "n_exploration_phases = 1\n",
    "\n",
    "deltas = [1e-4]*2\n",
    "\n",
    "inertias = [0.5]*2\n",
    "\n",
    "get_initial_state = lambda: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1aeaa4631945d695cae1f2709d228d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Qs, quantized_agent_policies, history = quantized_q_learning_alg(quantized_agent_policies, transition_kernel, get_initial_state,\n",
    "                reward_funcs, betas, T, alpha_func, n_exploration_phases, deltas, inertias, early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.60529981, -0.48286549],\n",
       "        [-0.5018332 , -0.39595302],\n",
       "        [-0.39702736, -0.35418681],\n",
       "        [-0.2869505 , -0.21852211],\n",
       "        [-0.07369052, -0.08097699],\n",
       "        [-0.07930672, -0.08747975],\n",
       "        [-0.17215772, -0.25154934],\n",
       "        [-0.34313679, -0.37399459],\n",
       "        [-0.35810128, -0.51415424],\n",
       "        [-0.58903872, -0.64107224]]),\n",
       " array([[-0.62740738, -0.65035598],\n",
       "        [-0.49869668, -0.54715235],\n",
       "        [-0.36451296, -0.41522599],\n",
       "        [-0.22623865, -0.28128899],\n",
       "        [-0.08957105, -0.07488306],\n",
       "        [-0.10114662, -0.08103516],\n",
       "        [-0.22815452, -0.2814276 ],\n",
       "        [-0.41423768, -0.36430974],\n",
       "        [-0.49759442, -0.54713628],\n",
       "        [-0.65038067, -0.62673511]])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0.0: 1,\n",
       "  0.1111111111111111: 1,\n",
       "  0.2222222222222222: 1,\n",
       "  0.3333333333333333: 1,\n",
       "  0.4444444444444444: -1,\n",
       "  0.5555555555555556: -1,\n",
       "  0.6666666666666666: -1,\n",
       "  0.7777777777777777: -1,\n",
       "  0.8888888888888888: -1,\n",
       "  1.0: -1},\n",
       " {0.0: -1,\n",
       "  0.1111111111111111: -1,\n",
       "  0.2222222222222222: -1,\n",
       "  0.3333333333333333: -1,\n",
       "  0.4444444444444444: 1,\n",
       "  0.5555555555555556: 1,\n",
       "  0.6666666666666666: -1,\n",
       "  0.7777777777777777: 1,\n",
       "  0.8888888888888888: -1,\n",
       "  1.0: 1})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_agent_policies[0].get_policy_map(), quantized_agent_policies[1].get_policy_map()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6344ff784fc1faea94d0e81da8b4b161117140e87b580ec4c92f28840fc25029"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

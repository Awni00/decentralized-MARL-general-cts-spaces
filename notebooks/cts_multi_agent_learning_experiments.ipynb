{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Quantized Decentralized Multi-Agent Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.getcwd())) \n",
    "\n",
    "from quantization import DiscreteSpace, ContinuousInterval, UniformQuantizer, NullQuantizer, QuantizedPolicy, TransitionKernel\n",
    "from cts_multi_agent_learning import quantized_q_learning_alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_REWARD_CONST = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trivial Single-Agent 'Finite' State and Action\n",
    "A simple MDP with a single state and two actions, one good one bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = DiscreteSpace([0]) # single state\n",
    "action_space = DiscreteSpace([0, 1]) # two actions\n",
    "\n",
    "state_quantizer = NullQuantizer(state_space)\n",
    "action_quantizer = NullQuantizer(action_space)\n",
    "\n",
    "q_policy = QuantizedPolicy(state_quantizer, action_quantizer, index_policy='random_init', exploration_prob=1)\n",
    "quantized_agent_policies = [q_policy]\n",
    "\n",
    "def transition_func(x, us):\n",
    "    return 0\n",
    "\n",
    "transition_kernel = TransitionKernel(state_space, [action_space], transition_func)\n",
    "\n",
    "def reward_func(x, us):\n",
    "    u = us[0]\n",
    "    if u==0:\n",
    "        return 0\n",
    "    elif u==1:\n",
    "        return POS_REWARD_CONST\n",
    "    else:\n",
    "        raise ValueError('received action outside expected range')\n",
    "\n",
    "reward_funcs = [reward_func]\n",
    "\n",
    "betas = [0.5]\n",
    "\n",
    "T = int(1e5)\n",
    "\n",
    "alpha_func = lambda n: 1/(n + 1)\n",
    "\n",
    "n_exploration_phases = 1\n",
    "\n",
    "deltas = [1e-4]\n",
    "\n",
    "inertias = [0]\n",
    "\n",
    "get_initial_state = lambda: 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699ec75de6b64ebdb4266eb06163b63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Qs, quantized_agent_policies, history = quantized_q_learning_alg(quantized_agent_policies, transition_kernel, get_initial_state,\n",
    "                reward_funcs, betas, T, alpha_func, n_exploration_phases, deltas, inertias, early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 98.99311272, 198.99176731]])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_agent_policies[0].get_policy_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slightly Less Trivial Single-Agent 'Finite' State and Action Spaces\n",
    "\n",
    "An MDP with a 'good' state and a 'bad' state, and two actions each of which causes a deterministic transition to one of the states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = DiscreteSpace(['good', 'bad']) # two state\n",
    "action_space = DiscreteSpace(['a', 'b']) # two actions\n",
    "\n",
    "state_quantizer = NullQuantizer(state_space)\n",
    "action_quantizer = NullQuantizer(action_space)\n",
    "\n",
    "q_policy = QuantizedPolicy(state_quantizer, action_quantizer, index_policy='random_init', exploration_prob=1)\n",
    "quantized_agent_policies = [q_policy]\n",
    "\n",
    "def transition_func(x, us):\n",
    "    u = us[0]\n",
    "    if u == 'a':\n",
    "        return 'good'\n",
    "    elif u=='b':\n",
    "        return 'bad'\n",
    "    else:\n",
    "        raise ValueError('received unexpected action')\n",
    "\n",
    "transition_kernel = TransitionKernel(state_space, [action_space], transition_func)\n",
    "\n",
    "def reward_func(x, us):\n",
    "    if x=='good':\n",
    "        return POS_REWARD_CONST\n",
    "    elif x=='bad':\n",
    "        return 0\n",
    "    else:\n",
    "        raise ValueError('received state/action outside expected range')\n",
    "\n",
    "reward_funcs = [reward_func]\n",
    "\n",
    "betas = [0.25]\n",
    "\n",
    "T = int(1e5)\n",
    "\n",
    "alpha_func = lambda n: 1/(n + 1)\n",
    "\n",
    "n_exploration_phases = 1\n",
    "\n",
    "deltas = [1e-4]\n",
    "\n",
    "inertias = [0]\n",
    "\n",
    "get_initial_state = lambda: 'good'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d3c0a53f424499ad7ddfaf4e65a367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Qs, quantized_agent_policies, history = quantized_q_learning_alg(quantized_agent_policies, transition_kernel, get_initial_state,\n",
    "                reward_funcs, betas, T, alpha_func, n_exploration_phases, deltas, inertias, early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[133.26780462, 108.27329238],\n",
       "        [ 33.26850317,   8.27629886]])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'good': 'a', 'bad': 'a'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_agent_policies[0].get_policy_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-agent Team (coordinate actions), 'Finite' State and action space\n",
    "\n",
    "A 2-agent generalization of the above where transition to the good state occurs if the agents coordinate (play the same action) and transition to the bad state occurs otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = DiscreteSpace(['good', 'bad']) # two state\n",
    "action_space1 = DiscreteSpace(['a', 'b']) # two actions\n",
    "action_space2 = DiscreteSpace(['a', 'b']) # two actions\n",
    "\n",
    "state_quantizer1 = NullQuantizer(state_space)\n",
    "state_quantizer2 = NullQuantizer(state_space)\n",
    "\n",
    "action_quantizer1 = NullQuantizer(action_space1)\n",
    "action_quantizer2 = NullQuantizer(action_space2)\n",
    "\n",
    "q_policy1 = QuantizedPolicy(state_quantizer1, action_quantizer1, index_policy='random_init', exploration_prob=0.25)\n",
    "q_policy2 = QuantizedPolicy(state_quantizer2, action_quantizer2, index_policy='random_init', exploration_prob=0.25)\n",
    "\n",
    "quantized_agent_policies = [q_policy1, q_policy2]\n",
    "\n",
    "def transition_func(x, us):\n",
    "    u1, u2 = us\n",
    "    \n",
    "    if u1 == u2:\n",
    "        return 'good'\n",
    "    elif u1 != u2:\n",
    "        return 'bad'\n",
    "    else:\n",
    "        raise ValueError('received unexpected action')\n",
    "\n",
    "transition_kernel = TransitionKernel(state_space, [action_space1, action_space2], transition_func)\n",
    "\n",
    "def reward_func(x, us):\n",
    "    if x=='good':\n",
    "        return POS_REWARD_CONST\n",
    "    elif x=='bad':\n",
    "        return 0\n",
    "    else:\n",
    "        raise ValueError('received state/action outside expected range')\n",
    "\n",
    "reward_funcs = [reward_func, reward_func]\n",
    "\n",
    "betas = [0.25]*2\n",
    "\n",
    "T = int(5e4)\n",
    "\n",
    "alpha_func = lambda n: 1/(n + 1)\n",
    "\n",
    "n_exploration_phases = 5\n",
    "\n",
    "deltas = [1e-4]*2\n",
    "\n",
    "inertias = [0.1]*2\n",
    "\n",
    "get_initial_state = lambda: 'good'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf16227f62647be99c50f1c91d48142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Qs, quantized_agent_policies, history = quantized_q_learning_alg(quantized_agent_policies, transition_kernel, get_initial_state,\n",
    "                reward_funcs, betas, T, alpha_func, n_exploration_phases, deltas, inertias, early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[118.2894217 , 110.94677187],\n",
       "        [  7.75999469,  29.12091001]]),\n",
       " array([[129.12874693, 110.25558867],\n",
       "        [ 29.04347357,  10.44216313]])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'good': 'a', 'bad': 'b'}, {'good': 'a', 'bad': 'a'})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_agent_policies[0].get_policy_map(), quantized_agent_policies[1].get_policy_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous State-Space Single-Agent\n",
    "A continuous space MDP over $\\mathbb{X} = [0,1]$ and $\\mathbb{U} = \\{-1, 1\\}$ where actions either transition the state forward or backward. A larger state value corresponds to higher reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = ContinuousInterval(0, 1) # single state\n",
    "action_space = DiscreteSpace([-1, 1]) # two actions\n",
    "\n",
    "state_quantizer = UniformQuantizer(state_space, n_bins=10)\n",
    "action_quantizer = NullQuantizer(action_space)\n",
    "\n",
    "q_policy = QuantizedPolicy(state_quantizer, action_quantizer, index_policy='random_init', exploration_prob=0.5)\n",
    "quantized_agent_policies = [q_policy]\n",
    "\n",
    "def transition_func(x, us):\n",
    "    u = us[0]\n",
    "    x_next = np.clip(x + 0.1*u, 0, 1)\n",
    "    return x_next\n",
    "\n",
    "transition_kernel = TransitionKernel(state_space, [action_space], transition_func)\n",
    "\n",
    "def reward_func(x, us):\n",
    "    return x*POS_REWARD_CONST # reward high state values\n",
    "\n",
    "reward_funcs = [reward_func]\n",
    "\n",
    "betas = [0.25]\n",
    "\n",
    "T = int(1e5)\n",
    "\n",
    "alpha_func = lambda n: 1/(n + 1)\n",
    "\n",
    "n_exploration_phases = 5\n",
    "\n",
    "deltas = [1e-4]\n",
    "\n",
    "inertias = [0]\n",
    "\n",
    "get_initial_state = lambda: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e3a6da98ca44fab631e463ed69f5ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Qs, quantized_agent_policies, history = quantized_q_learning_alg(quantized_agent_policies, transition_kernel, get_initial_state,\n",
    "                reward_funcs, betas, T, alpha_func, n_exploration_phases, deltas, inertias, early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[  0.        ,   0.41666667],\n",
       "        [  5.        ,   9.50751016],\n",
       "        [ 14.37934028,  22.82129757],\n",
       "        [ 27.36261215,  39.37083271],\n",
       "        [ 44.06385965,  58.55879826],\n",
       "        [ 75.70454055,  80.91144362],\n",
       "        [ 89.8528176 ,  97.54923891],\n",
       "        [104.18466483, 110.75914418],\n",
       "        [117.59525473, 123.285889  ],\n",
       "        [130.77709813, 133.28939707]])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 1,\n",
       " 0.1111111111111111: 1,\n",
       " 0.2222222222222222: 1,\n",
       " 0.3333333333333333: 1,\n",
       " 0.4444444444444444: 1,\n",
       " 0.5555555555555556: 1,\n",
       " 0.6666666666666666: 1,\n",
       " 0.7777777777777777: 1,\n",
       " 0.8888888888888888: 1,\n",
       " 1.0: 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_agent_policies[0].get_policy_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous State-Space Single-Agent\n",
    "\n",
    "Similar to the above except the cost measures the distance from $x = 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = ContinuousInterval(0, 1) # single state\n",
    "action_space = DiscreteSpace([-1, 1]) # two actions\n",
    "\n",
    "state_quantizer = UniformQuantizer(state_space, n_bins=10)\n",
    "action_quantizer = NullQuantizer(action_space)\n",
    "\n",
    "q_policy = QuantizedPolicy(state_quantizer, action_quantizer, index_policy='random_init', exploration_prob=1)\n",
    "quantized_agent_policies = [q_policy]\n",
    "\n",
    "def transition_func(x, us):\n",
    "    u = us[0]\n",
    "    x_next = np.clip(x + 0.1*u, 0, 1)\n",
    "    return x_next\n",
    "\n",
    "transition_kernel = TransitionKernel(state_space, [action_space], transition_func)\n",
    "\n",
    "def reward_func(x, us):\n",
    "    return -POS_REWARD_CONST * np.abs(x - 0.5) # reward state values near 0.5\n",
    "\n",
    "reward_funcs = [reward_func]\n",
    "\n",
    "betas = [0.25]\n",
    "\n",
    "T = int(1e5)\n",
    "\n",
    "alpha_func = lambda n: 1/(n + 1)\n",
    "\n",
    "n_exploration_phases = 1\n",
    "\n",
    "deltas = [1e-4]\n",
    "\n",
    "inertias = [0]\n",
    "\n",
    "get_initial_state = lambda: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2af8379620488a9f9009251adcd587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Qs, quantized_agent_policies, history = quantized_q_learning_alg(quantized_agent_policies, transition_kernel, get_initial_state,\n",
    "                reward_funcs, betas, T, alpha_func, n_exploration_phases, deltas, inertias, early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-65.46418463, -62.15435759],\n",
       "        [-55.47090179, -48.83929498],\n",
       "        [-42.15814533, -35.51487853],\n",
       "        [-28.8443483 , -22.16282302],\n",
       "        [-11.06777203,  -8.82974077],\n",
       "        [ -8.8943845 , -11.14661909],\n",
       "        [-22.22302084, -28.83774459],\n",
       "        [-35.5277977 , -42.14743418],\n",
       "        [-48.83763128, -55.46163846],\n",
       "        [-62.14984392, -65.4495664 ]])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 1,\n",
       " 0.1111111111111111: 1,\n",
       " 0.2222222222222222: 1,\n",
       " 0.3333333333333333: 1,\n",
       " 0.4444444444444444: 1,\n",
       " 0.5555555555555556: -1,\n",
       " 0.6666666666666666: -1,\n",
       " 0.7777777777777777: -1,\n",
       " 0.8888888888888888: -1,\n",
       " 1.0: -1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_agent_policies[0].get_policy_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous State-Space Single-Agent\n",
    "\n",
    "Similar to the above except the transitions aren't deterministic anymore and have a zero-mean gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = ContinuousInterval(0, 1) # single state\n",
    "action_space = DiscreteSpace([-1, 1]) # two actions\n",
    "\n",
    "state_quantizer = UniformQuantizer(state_space, n_bins=10)\n",
    "action_quantizer = NullQuantizer(action_space)\n",
    "\n",
    "q_policy = QuantizedPolicy(state_quantizer, action_quantizer, index_policy='random_init', exploration_prob=1)\n",
    "quantized_agent_policies = [q_policy]\n",
    "\n",
    "def transition_func(x, us):\n",
    "    u = us[0]\n",
    "    x_next_ = x + 0.1*u + np.random.normal(0, 0.05)\n",
    "    x_next = np.clip(x_next_, 0, 1)\n",
    "    return x_next\n",
    "\n",
    "transition_kernel = TransitionKernel(state_space, [action_space], transition_func)\n",
    "\n",
    "def reward_func(x, us):\n",
    "    return -POS_REWARD_CONST * np.abs(x - 0.5) # reward state values near 0.5\n",
    "\n",
    "reward_funcs = [reward_func]\n",
    "\n",
    "betas = [0.25]\n",
    "\n",
    "T = int(1e5)\n",
    "\n",
    "alpha_func = lambda n: 1/(n + 1)\n",
    "\n",
    "n_exploration_phases = 1\n",
    "\n",
    "deltas = [1e-4]\n",
    "\n",
    "inertias = [0]\n",
    "\n",
    "get_initial_state = lambda: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d202dcc7bf17467a8c94da673f6f26ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Qs, quantized_agent_policies, history = quantized_q_learning_alg(quantized_agent_policies, transition_kernel, get_initial_state,\n",
    "                reward_funcs, betas, T, alpha_func, n_exploration_phases, deltas, inertias, early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-64.25987612, -60.85763855],\n",
       "        [-52.99587546, -47.15328278],\n",
       "        [-39.14474641, -33.13096662],\n",
       "        [-24.5770633 , -19.22437353],\n",
       "        [-10.16424514,  -7.94833485],\n",
       "        [ -7.86499704, -10.18670733],\n",
       "        [-19.26458286, -24.69953497],\n",
       "        [-32.93313893, -39.13755279],\n",
       "        [-47.23366227, -52.98356422],\n",
       "        [-60.87010505, -64.21527874]])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 1,\n",
       " 0.1111111111111111: 1,\n",
       " 0.2222222222222222: 1,\n",
       " 0.3333333333333333: 1,\n",
       " 0.4444444444444444: 1,\n",
       " 0.5555555555555556: -1,\n",
       " 0.6666666666666666: -1,\n",
       " 0.7777777777777777: -1,\n",
       " 0.8888888888888888: -1,\n",
       " 1.0: -1}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_agent_policies[0].get_policy_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous State-Space Two-Agent\n",
    "\n",
    "A 2-agent with $\\mathbb{X} = [0,1]$ and $\\mathbb{U}^1 = \\mathbb{U}^2 = \\{-1, 1\\}$. Agents go forward by $0.1$ if the coordinate (play the same action), and go backwards by $0.1$ if they don't coordinate. The reward function rewards states closer to $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = ContinuousInterval(0, 1) # single state\n",
    "action_space1 = DiscreteSpace([-1, 1]) # two actions\n",
    "action_space2 = DiscreteSpace([-1, 1]) # two actions\n",
    "\n",
    "state_quantizer1 = UniformQuantizer(state_space, n_bins=5)\n",
    "state_quantizer2 = UniformQuantizer(state_space, n_bins=5)\n",
    "\n",
    "action_quantizer1 = NullQuantizer(action_space1)\n",
    "action_quantizer2 = NullQuantizer(action_space2)\n",
    "\n",
    "q_policy1 = QuantizedPolicy(state_quantizer1, action_quantizer1, index_policy='random_init', exploration_prob=0.2)\n",
    "q_policy2 = QuantizedPolicy(state_quantizer2, action_quantizer2, index_policy='random_init', exploration_prob=0.2)\n",
    "\n",
    "quantized_agent_policies = [q_policy1, q_policy2]\n",
    "\n",
    "def transition_func(x, us):\n",
    "    u1, u2 = us\n",
    "    d = u1 * u2 # forward if coordinating, backwards otherwise\n",
    "    x_next = np.clip(x + 0.1*d, 0, 1)\n",
    "    return x_next\n",
    "\n",
    "transition_kernel = TransitionKernel(state_space, [action_space1, action_space2], transition_func)\n",
    "\n",
    "def reward_func(x, us):\n",
    "    return POS_REWARD_CONST * x # reward high state values\n",
    "\n",
    "reward_funcs = [reward_func, reward_func]\n",
    "\n",
    "betas = [0.75]*2\n",
    "\n",
    "T = int(1e5)\n",
    "\n",
    "alpha_func = lambda n: 1/(n + 1)\n",
    "\n",
    "n_exploration_phases = 10\n",
    "\n",
    "deltas = [1e-4]*2\n",
    "\n",
    "inertias = [0.25, 0.75]\n",
    "\n",
    "get_initial_state = lambda: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270ae0b77c3e4da68bebfbd5b0eb2754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Qs, quantized_agent_policies, history = quantized_q_learning_alg(quantized_agent_policies, transition_kernel, get_initial_state,\n",
    "                reward_funcs, betas, T, alpha_func, n_exploration_phases, deltas, inertias, early_stopping=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[  4.37711946,   3.82312053],\n",
       "        [ 18.75      ,  21.015625  ],\n",
       "        [119.16926372, 268.92358012],\n",
       "        [247.49739128, 335.08580762],\n",
       "        [289.93759143, 374.87854579]]),\n",
       " array([[  4.48580665,   4.4800425 ],\n",
       "        [  0.        ,  24.375     ],\n",
       "        [259.76402871, 289.87736404],\n",
       "        [318.07480712, 341.7692545 ],\n",
       "        [364.62319351, 367.57987777]])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0.0: -1, 0.25: 1, 0.5: 1, 0.75: 1, 1.0: 1},\n",
       " {0.0: -1, 0.25: 1, 0.5: 1, 0.75: 1, 1.0: 1})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_agent_policies[0].get_policy_map(), quantized_agent_policies[1].get_policy_map()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6344ff784fc1faea94d0e81da8b4b161117140e87b580ec4c92f28840fc25029"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

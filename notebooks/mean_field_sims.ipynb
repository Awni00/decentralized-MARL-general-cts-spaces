{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b56c59be",
   "metadata": {},
   "source": [
    "## Mean-field Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c97586c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from multi_agent_learning import q_learning_alg1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81719b16",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b1bce",
   "metadata": {},
   "source": [
    "#### Problem Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7822a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 2 # number of states\n",
    "n_actions_const = 2 # all agents have the same action spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24eb2758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_state():\n",
    "    # initial state distribution\n",
    "    return np.random.choice(range(n_states)) # uniform over states for now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12c5516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_state(x, us):\n",
    "    '''returns next state given current state and action'''\n",
    "    \n",
    "    # switches state if majority of agents \"affirm\"\n",
    "    mean_u = np.average(us)\n",
    "    if mean_u > 0.5:\n",
    "        return (x+1)%2\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "992ad281",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 1\n",
    "def calc_alpha(n):\n",
    "    return 1/(n**r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fb314dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mf_reward(x, u_i, mean_u):\n",
    "    return float(x==1) # state 1 is desirable\n",
    "\n",
    "def gen_reward_i(i):\n",
    "    def reward_i(state, actions):\n",
    "        ui = actions[i]\n",
    "        mean_u = np.average(actions)\n",
    "        return mf_reward(state, ui, mean_u)\n",
    "    \n",
    "    return reward_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f68165ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # temp problem: this one didn't converge to correct sln... (is it weakly acyclic)\n",
    "# def tmp_reward(x, us):\n",
    "#     return float(x==1)\n",
    "\n",
    "\n",
    "# def tmp_transition_state(x, us):\n",
    "#     # switch to other state if u0 = 0, u1 = 1\n",
    "#     if us[0]==0 and us[1]==1:\n",
    "#         return (x+1)%2\n",
    "#     else: return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf78b48",
   "metadata": {},
   "source": [
    "#### N-Agent Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3bdc03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_solution(agent_policies):\n",
    "    state_0_correct = np.average(np.array(agent_policies)[:,0]) > 0.5\n",
    "    state_1_correct = np.average(np.array(agent_policies)[:, 1]) <= 0.5\n",
    "    return state_0_correct and state_1_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c15d1186",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_exploration_phases = 100 # number of exploration phases\n",
    "T = 1000 # length of exploration phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f2af12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# works (i.e.: finds optimal set of policies) for 1, 2, 3, 4, 5 agents \n",
    "# optimal policy not found for 6 or 7 within K=100; T=1000\n",
    "n_agents = 6 # number of agents\n",
    "\n",
    "n_Us = [n_actions_const] * n_agents # number of actions per agent (both agents have two actions)\n",
    "\n",
    "experimentation_probs = [0.005]*n_agents # probability of experimentation at each action (\\rho)\n",
    "inertias = [0.5] * n_agents # inertias of each agent (\\lambda)\n",
    "betas = [0.9]*n_agents # discount factor\n",
    "deltas = [1e-6]*n_agents # tolerance for suboptimality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af828309",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_funcs = [gen_reward_i(i) for i in range(n_agents)]\n",
    "# reward_funcs = [tmp_reward]*n_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67715de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.15it/s]\n"
     ]
    }
   ],
   "source": [
    "Qs, agent_policies, (policy_history, Qs_history, is_BR_history) = q_learning_alg1(n_Us, n_states, reward_funcs, betas,\n",
    "                                                                    get_initial_state, transition_state, \n",
    "                                                                    n_exploration_phases, T, experimentation_probs,\n",
    "                                                                    calc_alpha, deltas, inertias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d838d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1), (0, 1), (1, 1), (0, 1), (1, 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a111a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_solution(agent_policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a7e8ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([is_solution(p) for p in policy_history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01096a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True, False, True, False, True]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_BR_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f236c96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.43386376, 0.9255723 ],\n",
       "        [0.        , 1.22956401]]),\n",
       " array([[0.03596483, 0.03928572],\n",
       "        [0.        , 1.03077216]]),\n",
       " array([[0.79801055, 0.9       ],\n",
       "        [0.        , 1.6075    ]]),\n",
       " array([[0.03568341, 0.03928271],\n",
       "        [0.        , 1.03078034]]),\n",
       " array([[0.55331219, 0.92144118],\n",
       "        [0.        , 1.42656029]]),\n",
       " array([[0.02920546, 0.04129203],\n",
       "        [0.        , 1.03251056]])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ad47f5",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5493732",
   "metadata": {},
   "source": [
    "#### Problem Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd6ff89",
   "metadata": {},
   "source": [
    "same problem as above except the transition now probabilistically depends on `mean_u` rather than being threshold-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34089ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 2 # number of states\n",
    "n_actions_const = 2 # all agents have the same action spaces\n",
    "\n",
    "def get_initial_state():\n",
    "    # initial state distribution\n",
    "    return np.random.choice(range(n_states)) # uniform over states for now \n",
    "\n",
    "def transition_state(x, us):\n",
    "    '''returns next state given current state and action'''\n",
    "    \n",
    "    # switches state with probability <u>\n",
    "    mean_u = np.average(us)\n",
    "    if np.random.random() < mean_u :\n",
    "        return (x+1)%2\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "r = 1\n",
    "def calc_alpha(n):\n",
    "    return 1/(n**r)\n",
    "\n",
    "def mf_reward(x, u_i, mean_u):\n",
    "    return float(x==1) # state 1 is desirable\n",
    "\n",
    "def gen_reward_i(i):\n",
    "    def reward_i(state, actions):\n",
    "        ui = actions[i]\n",
    "        mean_u = np.average(actions)\n",
    "        return mf_reward(state, ui, mean_u)\n",
    "    \n",
    "    return reward_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a71aa6e",
   "metadata": {},
   "source": [
    "#### N-agent Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b404902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: get's closer to team optimal policy than problem 1, \n",
    "# but requires tinkering with parameters to reach the single optimal policy\n",
    "# for larger N, convergence to the optimal policy becomes harder, but we remain around a near-optimal policy\n",
    "\n",
    "# note also: depending on transition kernel, a mean-field formulation may result in low visitation to certain subset of \n",
    "# states, thus affecting convergence and learned Q-factors and policies for those states\n",
    "# e.g.: in this case, if in state 1, and all actions are 0 in state 1, won't ever visit state 0 to learn the optimal policy\n",
    "# and visitation to state 0 in that case requires exploration and has low probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a79ff1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_exploration_phases = 100 # number of exploration phases\n",
    "T = 10000 # length of exploration phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39f52d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  w/ T=1000, K=100 found team optimal policies for n=2, 3\n",
    "# w/ T=10,000 K=100 found team optimal policies for n=4\n",
    "n_agents = 6 # number of agents\n",
    "\n",
    "n_Us = [n_actions_const] * n_agents # number of actions per agent (both agents have two actions)\n",
    "\n",
    "experimentation_probs = [0.05]*n_agents # probability of experimentation at each action (\\rho)\n",
    "inertias = [0.5] * n_agents # inertias of each agent (\\lambda)\n",
    "betas = [0.9]*n_agents # discount factor\n",
    "deltas = [1e-6]*n_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99727d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_funcs = [gen_reward_i(i) for i in range(n_agents)]\n",
    "# reward_funcs = [tmp_reward]*n_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1366a191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [12:08<00:00,  7.29s/it]\n"
     ]
    }
   ],
   "source": [
    "Qs, agent_policies, (policy_history, Qs_history, is_BR_history) = q_learning_alg1(n_Us, n_states, reward_funcs, betas,\n",
    "                                                                    get_initial_state, transition_state, \n",
    "                                                                    n_exploration_phases, T, experimentation_probs,\n",
    "                                                                    calc_alpha, deltas, inertias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "992168cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 1), (1, 0), (1, 1), (1, 1), (1, 0)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7008dfb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0), (1, 1), (1, 0), (0, 0), (0, 1), (0, 0)],\n",
       " [(0, 0), (1, 0), (1, 0), (1, 0), (0, 1), (1, 0)],\n",
       " [(0, 0), (1, 0), (1, 0), (1, 0), (0, 1), (1, 0)],\n",
       " [(0, 0), (1, 0), (1, 0), (1, 0), (0, 0), (1, 0)],\n",
       " [(1, 1), (1, 0), (1, 0), (1, 0), (0, 0), (1, 0)],\n",
       " [(1, 1), (1, 0), (1, 0), (1, 0), (0, 0), (1, 0)],\n",
       " [(1, 1), (1, 0), (1, 0), (1, 0), (0, 0), (1, 0)],\n",
       " [(1, 0), (1, 0), (1, 0), (1, 0), (0, 0), (1, 0)],\n",
       " [(1, 0), (1, 0), (1, 1), (1, 0), (0, 1), (1, 0)],\n",
       " [(1, 0), (1, 0), (1, 1), (0, 0), (0, 1), (1, 0)]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_history[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90aa1ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 1)],\n",
       " [(0, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 1)],\n",
       " [(0, 0), (1, 1), (1, 0), (1, 1), (1, 0), (1, 0)],\n",
       " [(1, 0), (1, 1), (1, 0), (1, 1), (1, 0), (0, 0)],\n",
       " [(1, 0), (1, 1), (1, 0), (1, 1), (1, 0), (0, 0)],\n",
       " [(1, 0), (1, 1), (0, 0), (1, 1), (1, 0), (1, 0)],\n",
       " [(0, 0), (1, 1), (0, 0), (0, 0), (0, 0), (1, 0)],\n",
       " [(0, 0), (1, 1), (1, 0), (0, 0), (0, 1), (1, 0)],\n",
       " [(0, 0), (1, 0), (1, 0), (1, 0), (1, 1), (1, 0)],\n",
       " [(0, 0), (1, 0), (1, 0), (1, 0), (1, 1), (1, 0)]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_history[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f697ed4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[2.96220791, 0.        ],\n",
       "        [6.61889065, 6.60564673]]),\n",
       " array([[0.        , 3.02008413],\n",
       "        [6.73683547, 6.74328949]]),\n",
       " array([[0.        , 2.97083292],\n",
       "        [6.63824011, 6.56825866]]),\n",
       " array([[0.        , 2.89305379],\n",
       "        [6.46679011, 6.47451507]]),\n",
       " array([[0.        , 2.90334128],\n",
       "        [6.48251763, 6.48965127]]),\n",
       " array([[0.        , 2.93422965],\n",
       "        [6.51658295, 6.56714222]])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf0d408",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd6e285",
   "metadata": {},
   "source": [
    "#### Problem Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f15450",
   "metadata": {},
   "source": [
    "same problem but now with base probability to switch state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "11757ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 2 # number of states\n",
    "n_actions_const = 2 # all agents have the same action spaces\n",
    "\n",
    "def get_initial_state():\n",
    "    # initial state distribution\n",
    "    return np.random.choice(range(n_states)) # uniform over states for now \n",
    "\n",
    "base_switch_prob = 0.1\n",
    "def transition_state(x, us):\n",
    "    '''returns next state given current state and action'''\n",
    "    \n",
    "    # switch state regardless of input at a base probability\n",
    "    # switches state with probability <u>\n",
    "    mean_u = np.average(us)\n",
    "    if np.random.random() < max(0.1, mean_u) :\n",
    "        return (x+1)%2\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "r = 1\n",
    "def calc_alpha(n):\n",
    "    return 1/(n**r)\n",
    "\n",
    "def mf_reward(x, u_i, mean_u):\n",
    "    return float(x==1) # state 1 is desirable\n",
    "\n",
    "def gen_reward_i(i):\n",
    "    def reward_i(state, actions):\n",
    "        ui = actions[i]\n",
    "        mean_u = np.average(actions)\n",
    "        return mf_reward(state, ui, mean_u)\n",
    "    \n",
    "    return reward_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7933b42",
   "metadata": {},
   "source": [
    "#### N-agent sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ab0eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for large N, and this choice of parameters, we arrive at near-optimal policies, but not fully optimal policies\n",
    "# why? would we arrive at the optimal policy for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "563ee35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_exploration_phases = 100 # number of exploration phases\n",
    "T = 10000 # length of exploration phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9febb229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converged to team optimal policy for n=\n",
    "n_agents = 8 # number of agents\n",
    "\n",
    "n_Us = [n_actions_const] * n_agents # number of actions per agent (both agents have two actions)\n",
    "\n",
    "experimentation_probs = [0.01]*n_agents # probability of experimentation at each action (\\rho)\n",
    "inertias = [0.5] * n_agents # inertias of each agent (\\lambda)\n",
    "betas = [0.9]*n_agents # discount factor\n",
    "deltas = [1e-6]*n_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3ddb98ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_funcs = [gen_reward_i(i) for i in range(n_agents)]\n",
    "# reward_funcs = [tmp_reward]*n_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a7fee275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 100/100 [06:06<00:00,  3.66s/it]\n"
     ]
    }
   ],
   "source": [
    "Qs, agent_policies, (policy_history, Qs_history, is_BR_history) = q_learning_alg1(n_Us, n_states, reward_funcs, betas,\n",
    "                                                                    get_initial_state, transition_state, \n",
    "                                                                    n_exploration_phases, T, experimentation_probs,\n",
    "                                                                    calc_alpha, deltas, inertias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "caf6f284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0), (1, 0), (1, 0), (1, 0), (0, 0), (0, 0), (1, 0), (0, 0)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3352b0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy = [(1, 0) for _ in range(n_agents)]\n",
    "\n",
    "# % of time spent at optimal policy\n",
    "np.average([p == optimal_policy for p in policy_history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5fce68b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1, 0), (1, 0), (1, 0), (1, 0), (1, 1), (1, 0), (1, 1), (1, 0)],\n",
       " [(1, 1), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 1), (1, 0)],\n",
       " [(1, 0), (1, 1), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0)],\n",
       " [(0, 0), (1, 1), (0, 0), (0, 0), (1, 0), (0, 0), (0, 0), (1, 0)],\n",
       " [(0, 0), (1, 1), (0, 0), (0, 0), (1, 0), (0, 0), (1, 0), (1, 0)],\n",
       " [(1, 0), (1, 1), (0, 0), (1, 0), (1, 0), (0, 0), (1, 0), (1, 0)],\n",
       " [(1, 0), (1, 1), (0, 0), (1, 0), (0, 0), (0, 0), (1, 0), (1, 0)],\n",
       " [(0, 0), (1, 1), (1, 1), (0, 0), (0, 0), (1, 0), (1, 0), (1, 0)],\n",
       " [(0, 0), (1, 1), (1, 1), (0, 0), (0, 0), (1, 0), (1, 0), (0, 0)],\n",
       " [(0, 0), (1, 0), (0, 1), (1, 0), (0, 0), (1, 0), (1, 0), (0, 0)]]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_history[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "439ff15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[False, False, True, True, False, True, False, True],\n",
       " [False, False, True, True, True, True, False, True],\n",
       " [False, False, False, False, False, False, False, True],\n",
       " [False, True, False, False, True, False, False, True],\n",
       " [False, True, False, False, True, True, True, False],\n",
       " [True, False, False, True, False, False, True, True],\n",
       " [False, False, False, False, True, False, True, True],\n",
       " [False, False, False, True, True, True, True, False],\n",
       " [False, False, False, False, False, True, True, True],\n",
       " [False, True, False, False, True, False, False, False]]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_BR_history[-10:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bb72814",
   "metadata": {},
   "source": [
    "## Simulations from AY'17 Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e55e0a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5450ca",
   "metadata": {},
   "source": [
    "### Simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "125eeafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_state():\n",
    "    # initial state distribution\n",
    "    return np.random.choice(range(n_states)) # uniform over states for now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c9f72a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cost_func(cost_matrix, i):\n",
    "    '''create a cost function : X, U -> cost from `cost_matrix`'''\n",
    "    # TODO: fix. cost depends only on \n",
    "    \n",
    "    def cost_func(x, us):\n",
    "        u = us[i]\n",
    "        return cost_matrix[x,u]\n",
    "\n",
    "    return cost_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3413e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = -1, 2, 1\n",
    "reward_matrix = np.array([[c, a],[b, 0]])\n",
    "\n",
    "def reward0(state, actions):\n",
    "    ui, u_i = actions\n",
    "    return reward_matrix[ui, u_i]\n",
    "def reward1(state, actions):\n",
    "    u_i, ui = actions\n",
    "    return reward_matrix[ui, u_i]\n",
    "\n",
    "reward_funcs = [reward0, reward1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90d49e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_state(x, us):\n",
    "    '''returns next state given current state and action'''\n",
    "    gamma = 0.3\n",
    "\n",
    "    if us == [0,0]:\n",
    "        if np.random.random() < 1 - gamma:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        if np.random.random() < 1 - gamma:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ccda0273",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0.751 # 0.51\n",
    "def calc_alpha(n):\n",
    "    return 1/(n**r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f900a9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 2 # number of states\n",
    "n_agents = 2 # number of agents\n",
    "\n",
    "n_Us = [2] * n_agents # number of actions per agent (both agents have two actions)\n",
    "\n",
    "experimentation_probs = [0.1]*n_agents # probability of experimentation at each action (\\rho)\n",
    "inertias = [0.5] * n_agents # inertias of each agent (\\lambda)\n",
    "betas = [0.9]*n_agents # discount factor\n",
    "deltas = [0]*n_agents # (in paper it's 0) [this is critical; does T depend on this?]\n",
    "n_exploration_phases = 100 # number of exploration phases\n",
    "T = 100 # length of exploration phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3350ff7f",
   "metadata": {},
   "source": [
    "### Run Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9ce502f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_agent_learning import q_learning_alg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "83f30972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 100/100 [00:00<00:00, 169.11it/s]\n"
     ]
    }
   ],
   "source": [
    "Qs, agent_policies, (policy_history, Qs_history, is_BR_history) = q_learning_alg1(n_Us, n_states, reward_funcs, betas,\n",
    "                                                                    get_initial_state, transition_state, \n",
    "                                                                    n_exploration_phases, T, experimentation_probs,\n",
    "                                                                    calc_alpha, deltas, inertias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c553f139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.04036487, 0.96075697],\n",
       "        [0.46607843, 1.00253599]]),\n",
       " array([[-0.46195971,  0.65680714],\n",
       "        [-0.31045334,  0.6038834 ]])]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cf4a7091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (1, 1)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f2938916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1, 0), (1, 1)],\n",
       " [(1, 0), (1, 1)],\n",
       " [(1, 0), (1, 1)],\n",
       " [(1, 1), (1, 1)],\n",
       " [(1, 1), (1, 1)],\n",
       " [(1, 1), (1, 1)],\n",
       " [(1, 1), (1, 1)],\n",
       " [(1, 1), (1, 1)],\n",
       " [(1, 1), (1, 1)],\n",
       " [(1, 1), (1, 1)]]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_history[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "50b6f442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(np.all(is_BR_history, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d20398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: \n",
    "# behavior roughly the same in terms of `fraction of time joint policy (mu1(k),mu2(k)) is at an MPE` for both sets of code\n",
    "# slight deviation from trial to trial depending on initial random joint policy\n",
    "# at K=100, T=10,000, rho=0.01, fraction of time joint policy (mu1(k),mu2(k)) is at an MPE was 0.98 for both sets of code\n",
    "# at K=100, T=1,000, rho=0.01, fraction of time joint policy (mu1(k),mu2(k)) is at an MPE was 0.97 for both sets of code\n",
    "# at K=100, T=1,000, rho=0.1, fraction of time joint policy (mu1(k),mu2(k)) is at an MPE was 0.96 for both sets of code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
